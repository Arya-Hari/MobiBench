{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1S5tKGWbK-I60LWOCMooNcJLYLCOslDHk","authorship_tag":"ABX9TyOfG/2WXiWhDBPJT8WNk0F3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtrz3RvgmOUN","executionInfo":{"status":"ok","timestamp":1757827891923,"user_tz":-330,"elapsed":7950,"user":{"displayName":"ARYA HARIHARAN","userId":"06751263899597886017"}},"outputId":"55293247-86a2-402f-97bf-a8077e6cddfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading drive/MyDrive/Samsung - Benchmarking/data/original-summarization/test-00000-of-00001 (1).parquet ...\n","Reading drive/MyDrive/Samsung - Benchmarking/data/original-summarization/test-00000-of-00001 (2).parquet ...\n","Reading drive/MyDrive/Samsung - Benchmarking/data/original-summarization/test-00000-of-00001 (3).parquet ...\n","Final dataset shape: (1002, 3)\n","Saved combined dataset to summarization_dataset.parquet\n"]}],"source":["# Extract 200 random entries each from five .parquet files, creating a 1000 entry dataset\n","import pandas as pd\n","import glob\n","import os\n","\n","def sample_and_combine(parquet_files, samples_per_file=334, output_file=\"sampled_dataset.parquet\"):\n","    \"\"\"\n","    Reads multiple parquet files, randomly samples rows from each,\n","    and combines them into a single parquet dataset.\n","\n","    Args:\n","        parquet_files (list): List of parquet file paths.\n","        samples_per_file (int): Number of random samples to select from each file.\n","        output_file (str): Path for the combined parquet output file.\n","    \"\"\"\n","    sampled_dfs = []\n","\n","    for file in parquet_files:\n","        print(f\"Reading {file} ...\")\n","        df = pd.read_parquet(file)\n","        sampled = df.sample(n=samples_per_file, random_state=42)  # for reproducibility\n","        sampled_dfs.append(sampled)\n","\n","    combined_df = pd.concat(sampled_dfs, ignore_index=True)\n","    print(f\"Final dataset shape: {combined_df.shape}\")\n","\n","    combined_df.to_parquet(output_file, index=False)\n","    print(f\"Saved combined dataset to {output_file}\")\n","\n","\n","if __name__ == \"__main__\":\n","    # Adjust pattern or file paths as needed\n","    parquet_files = sorted(glob.glob(\"drive/MyDrive/Samsung - Benchmarking/data/original-summarization/*.parquet\"))[:3]  # first 5 parquet files in 'data/' directory\n","\n","    sample_and_combine(parquet_files, samples_per_file=334, output_file=\"summarization_dataset.parquet\")\n"]},{"cell_type":"code","source":["# Print size of a dataframe\n","import pandas as pd\n","import glob\n","import os\n","\n","file = glob.glob(\"drive/MyDrive/Samsung - Benchmarking/data/original-mmlu/*.parquet\")\n","print(f\"Reading {file} ...\")\n","df = pd.read_parquet(file)\n","print(df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOq1PBgzpb2G","executionInfo":{"status":"ok","timestamp":1757829896810,"user_tz":-330,"elapsed":1614,"user":{"displayName":"ARYA HARIHARAN","userId":"06751263899597886017"}},"outputId":"90c40044-cc81-4084-9eda-f88282104ac1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading ['drive/MyDrive/Samsung - Benchmarking/data/original-mmlu/test-00000-of-00001.parquet'] ...\n","(14042, 4)\n"]}]},{"cell_type":"code","source":["# Extract 1000 random entries from a single .parquet file\n","import pandas as pd\n","import glob\n","import os\n","\n","file = glob.glob(\"drive/MyDrive/Samsung - Benchmarking/data/original-mmlu/*.parquet\")\n","print(f\"Reading {file} ...\")\n","df = pd.read_parquet(file)\n","print(df.shape)\n","\n","output_file = \"mmlu_dataset.parquet\"\n","sampled = df.sample(n=1000, random_state=42)\n","sampled.to_parquet(output_file, index=False)\n","print(f\"Saved combined dataset to {output_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wFgmwsF0967","executionInfo":{"status":"ok","timestamp":1757830008895,"user_tz":-330,"elapsed":155,"user":{"displayName":"ARYA HARIHARAN","userId":"06751263899597886017"}},"outputId":"b0db9caf-bd6f-424e-accd-6b6d19eb1316"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading ['drive/MyDrive/Samsung - Benchmarking/data/original-mmlu/test-00000-of-00001.parquet'] ...\n","(14042, 4)\n","Saved combined dataset to mmlu_dataset.parquet\n"]}]},{"cell_type":"code","source":["# Convert parquet to CSV\n","import pandas as pd\n","\n","# Read parquet file\n","df = pd.read_parquet(\"drive/MyDrive/Samsung - Benchmarking/data/science_qa_dataset.parquet\")\n","\n","# Save as CSV\n","df.to_csv(\"drive/MyDrive/Samsung - Benchmarking/data/science_qa_dataset.csv\", index=False)\n"],"metadata":{"id":"SgAwWfRg1xq5","executionInfo":{"status":"ok","timestamp":1757830348625,"user_tz":-330,"elapsed":484,"user":{"displayName":"ARYA HARIHARAN","userId":"06751263899597886017"}}},"execution_count":11,"outputs":[]}]}